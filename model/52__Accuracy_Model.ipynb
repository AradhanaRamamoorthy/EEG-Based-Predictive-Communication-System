{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_ZjaC7SabZJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Data/\""
      ],
      "metadata": {
        "id": "yPqCdK8xamj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(data_path + 'train_data.npy')\n",
        "test_data = np.load(data_path + 'test_data.npy')\n",
        "train_label = np.load(data_path + 'train_label.npy')\n",
        "test_label = np.load(data_path + 'test_label.npy')"
      ],
      "metadata": {
        "id": "12mMBGQIaoei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To convert the data into PyTorch tensors\n",
        "x_train_tensor = torch.Tensor(train_data)\n",
        "y_train_tensor = torch.LongTensor(train_label)\n",
        "x_test_tensor = torch.Tensor(test_data)\n",
        "y_test_tensor = torch.LongTensor(test_label)"
      ],
      "metadata": {
        "id": "ctzMcmIuarLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your training data\n",
        "x_train = np.load(data_path + 'train_data.npy')\n",
        "y_train = np.load(data_path + 'train_label.npy')\n"
      ],
      "metadata": {
        "id": "kNtbLHOjnAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "x_train_tensor = torch.Tensor(x_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n"
      ],
      "metadata": {
        "id": "aTrSd2ZMm7qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Setting GPU on your computer"
      ],
      "metadata": {
        "id": "zyTwwdk3arvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(x_train_tensor.to(device), y_train_tensor.to(device)) # input data to Tensor dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True) #  Batch size refers to the number of data sample\n",
        "test_dataset = TensorDataset(x_test_tensor.to(device), y_test_tensor.to(device))\n",
        "test_loader = DataLoader(test_dataset, batch_size=64,  drop_last=True,shuffle=False)"
      ],
      "metadata": {
        "id": "P3iGP8snauSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGAutoencoderClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(EEGAutoencoderClassifier, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(64 * 795, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Define a list of classes corresponding to the predicted indices\n",
        "classes = [\"hello\", \"help me\", \"stop\", \"thank you\", \"yes\"]\n",
        "\n",
        "num_classes = 5\n",
        "model = EEGAutoencoderClassifier(num_classes).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 523\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "ADXo1gR4awza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_words = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Convert predicted indices to words\n",
        "        predicted_words.extend([classes[i] for i in predicted])\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "print(\"Predicted words:\", predicted_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM584jQBayxQ",
        "outputId": "4d6831f5-4e58-4d8b-effe-7c9bad8bd282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 52.13%\n",
            "Predicted words: ['help me', 'stop', 'help me', 'stop', 'yes', 'yes', 'help me', 'stop', 'stop', 'hello', 'stop', 'yes', 'yes', 'help me', 'help me', 'stop', 'help me', 'yes', 'stop', 'stop', 'stop', 'stop', 'help me', 'stop', 'stop', 'hello', 'hello', 'stop', 'stop', 'help me', 'stop', 'thank you', 'yes', 'thank you', 'yes', 'help me', 'stop', 'yes', 'yes', 'stop', 'stop', 'thank you', 'yes', 'stop', 'help me', 'stop', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'yes', 'help me', 'thank you', 'stop', 'thank you', 'stop', 'yes', 'thank you', 'yes', 'yes', 'yes', 'hello', 'yes', 'hello', 'hello', 'hello', 'thank you', 'yes', 'hello', 'stop', 'yes', 'stop', 'yes', 'thank you', 'stop', 'yes', 'yes', 'stop', 'stop', 'hello', 'help me', 'stop', 'stop', 'stop', 'stop', 'yes', 'stop', 'thank you', 'stop', 'help me', 'thank you', 'thank you', 'stop', 'stop', 'hello', 'thank you', 'yes', 'yes', 'thank you', 'thank you', 'thank you', 'yes', 'thank you', 'thank you', 'hello', 'hello', 'stop', 'hello', 'hello', 'stop', 'help me', 'yes', 'help me', 'stop', 'stop', 'yes', 'yes', 'thank you', 'help me', 'thank you', 'help me', 'yes', 'thank you', 'yes', 'hello', 'stop', 'stop', 'thank you', 'stop', 'help me', 'help me', 'stop', 'yes', 'help me', 'stop', 'help me', 'yes', 'yes', 'stop', 'stop', 'help me', 'stop', 'yes', 'hello', 'hello', 'stop', 'hello', 'yes', 'stop', 'help me', 'help me', 'thank you', 'stop', 'stop', 'help me', 'stop', 'help me', 'help me', 'hello', 'stop', 'thank you', 'hello', 'stop', 'stop', 'thank you', 'stop', 'yes', 'hello', 'hello', 'thank you', 'yes', 'thank you', 'hello', 'hello', 'help me', 'yes', 'hello', 'stop', 'stop', 'help me', 'hello', 'thank you', 'help me', 'hello', 'stop', 'thank you', 'stop', 'stop', 'stop', 'hello', 'help me', 'yes', 'help me', 'stop', 'yes', 'hello', 'hello', 'yes', 'stop', 'yes', 'yes', 'stop', 'thank you', 'help me', 'thank you', 'stop', 'stop', 'hello', 'hello', 'hello', 'help me', 'help me', 'hello', 'help me', 'yes', 'yes', 'yes', 'help me', 'yes', 'stop', 'thank you', 'thank you', 'yes', 'stop', 'help me', 'yes', 'help me', 'stop', 'thank you', 'stop', 'stop', 'yes', 'thank you', 'stop', 'yes', 'yes', 'stop', 'thank you', 'hello', 'help me', 'stop', 'yes', 'hello', 'help me', 'yes', 'stop', 'hello', 'hello', 'stop', 'help me', 'help me', 'help me', 'help me', 'yes', 'hello', 'thank you', 'stop', 'stop', 'hello', 'stop', 'yes', 'stop', 'yes', 'help me', 'help me', 'thank you', 'yes', 'help me', 'stop', 'stop', 'stop', 'help me', 'stop', 'thank you', 'thank you', 'help me', 'hello', 'thank you', 'hello', 'help me', 'help me', 'yes', 'hello', 'help me', 'stop', 'thank you', 'thank you', 'yes', 'stop', 'thank you', 'yes', 'stop', 'yes', 'thank you', 'thank you', 'yes', 'yes', 'stop', 'thank you', 'hello', 'stop', 'hello', 'hello', 'yes', 'yes', 'yes', 'hello', 'thank you', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'help me', 'help me', 'hello', 'help me', 'hello', 'thank you', 'stop', 'stop', 'stop', 'thank you', 'help me', 'stop', 'stop', 'yes', 'yes', 'hello', 'thank you', 'yes', 'thank you', 'stop', 'help me', 'stop', 'help me', 'stop', 'hello', 'thank you', 'hello', 'yes', 'stop', 'yes', 'stop', 'thank you', 'thank you', 'thank you', 'hello', 'stop', 'stop', 'help me', 'hello', 'help me', 'stop', 'thank you', 'yes', 'help me', 'hello', 'stop', 'help me', 'stop', 'stop', 'yes', 'stop', 'thank you', 'yes', 'help me', 'thank you', 'hello', 'yes', 'hello', 'hello', 'stop', 'thank you', 'stop', 'help me', 'yes', 'thank you', 'hello', 'thank you', 'hello', 'thank you', 'help me', 'stop', 'help me', 'stop', 'stop', 'hello', 'stop', 'yes', 'stop', 'stop', 'thank you', 'yes', 'help me', 'hello', 'thank you', 'stop', 'stop', 'help me', 'yes', 'yes', 'thank you', 'yes', 'stop', 'thank you', 'thank you', 'stop', 'help me', 'thank you', 'stop', 'hello', 'stop', 'yes', 'stop', 'yes', 'thank you', 'help me', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'stop', 'thank you', 'hello', 'stop', 'thank you', 'stop', 'thank you', 'hello', 'yes', 'stop', 'yes', 'thank you', 'hello', 'help me', 'hello', 'help me', 'stop', 'hello', 'hello', 'help me', 'yes', 'help me', 'thank you', 'yes', 'help me', 'hello', 'thank you', 'thank you', 'stop', 'stop', 'thank you', 'help me', 'help me', 'hello', 'stop', 'thank you', 'yes', 'hello', 'yes', 'thank you', 'hello', 'yes', 'yes', 'thank you', 'stop', 'stop', 'stop', 'yes', 'help me', 'yes', 'hello', 'stop', 'thank you', 'help me', 'hello', 'thank you', 'stop', 'stop', 'thank you', 'hello', 'help me', 'hello', 'thank you', 'thank you', 'stop', 'help me', 'thank you', 'hello', 'thank you', 'help me', 'help me', 'hello', 'stop', 'thank you', 'stop', 'stop', 'hello', 'stop', 'stop', 'stop', 'stop', 'hello', 'hello', 'stop', 'thank you', 'stop', 'stop', 'yes', 'hello', 'stop', 'yes', 'stop', 'yes', 'stop', 'help me', 'yes', 'stop', 'stop', 'help me', 'help me', 'hello', 'help me', 'hello', 'hello', 'yes', 'help me', 'help me', 'stop', 'stop', 'hello', 'thank you', 'thank you', 'hello', 'thank you', 'hello', 'hello', 'thank you', 'hello', 'stop', 'stop', 'hello', 'thank you', 'help me', 'help me', 'help me', 'help me', 'yes', 'yes', 'yes', 'help me', 'help me', 'thank you', 'thank you', 'stop', 'stop', 'hello', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you', 'yes', 'help me', 'thank you', 'stop', 'thank you', 'hello', 'stop', 'yes', 'thank you', 'hello', 'yes', 'hello', 'hello', 'hello', 'stop', 'hello', 'help me', 'help me', 'stop', 'stop', 'thank you', 'help me', 'hello', 'stop', 'yes', 'hello', 'stop', 'hello', 'hello', 'stop', 'hello', 'yes', 'hello', 'thank you', 'thank you', 'thank you', 'stop', 'yes', 'hello', 'help me', 'yes', 'stop', 'thank you', 'stop', 'hello', 'yes', 'help me', 'yes', 'stop', 'thank you', 'help me', 'hello', 'stop', 'stop', 'yes', 'stop', 'hello', 'hello', 'hello', 'stop', 'yes', 'thank you', 'hello', 'thank you', 'yes', 'stop', 'stop', 'thank you', 'stop', 'yes', 'help me', 'stop', 'yes', 'help me', 'yes', 'stop', 'stop', 'stop', 'thank you', 'help me', 'thank you', 'stop', 'hello', 'yes', 'hello', 'yes', 'stop', 'stop', 'hello', 'stop', 'stop', 'stop', 'thank you', 'help me', 'help me', 'help me', 'stop', 'thank you', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'yes', 'hello', 'yes', 'yes', 'stop', 'stop', 'help me', 'hello', 'stop', 'thank you', 'help me', 'stop', 'help me', 'yes', 'hello', 'stop', 'help me', 'hello', 'yes', 'yes', 'yes', 'yes', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "i1i-kS2Kk72a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open('/content/drive/MyDrive/SavedModel', 'wb'))   #saving the trained model"
      ],
      "metadata": {
        "id": "tnjlZ1HOlfOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = pickle.load(open('/content/drive/MyDrive/SavedModel', 'rb'))  #openig the traied model"
      ],
      "metadata": {
        "id": "M4HMoEijl6oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded.eval()         # evaluating loaded model accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "predicted_words = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Convert predicted indices to words\n",
        "        predicted_words.extend([classes[i] for i in predicted])\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "print(\"Predicted words:\", predicted_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gohtsKR_mDB3",
        "outputId": "9c80a854-5cec-442a-efe4-87e175b4d032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 52.13%\n",
            "Predicted words: ['help me', 'stop', 'help me', 'stop', 'yes', 'yes', 'help me', 'stop', 'stop', 'hello', 'stop', 'yes', 'yes', 'help me', 'help me', 'stop', 'help me', 'yes', 'stop', 'stop', 'stop', 'stop', 'help me', 'stop', 'stop', 'hello', 'hello', 'stop', 'stop', 'help me', 'stop', 'thank you', 'yes', 'thank you', 'yes', 'help me', 'stop', 'yes', 'yes', 'stop', 'stop', 'thank you', 'yes', 'stop', 'help me', 'stop', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'yes', 'help me', 'thank you', 'stop', 'thank you', 'stop', 'yes', 'thank you', 'yes', 'yes', 'yes', 'hello', 'yes', 'hello', 'hello', 'hello', 'thank you', 'yes', 'hello', 'stop', 'yes', 'stop', 'yes', 'thank you', 'stop', 'yes', 'yes', 'stop', 'stop', 'hello', 'help me', 'stop', 'stop', 'stop', 'stop', 'yes', 'stop', 'thank you', 'stop', 'help me', 'thank you', 'thank you', 'stop', 'stop', 'hello', 'thank you', 'yes', 'yes', 'thank you', 'thank you', 'thank you', 'yes', 'thank you', 'thank you', 'hello', 'hello', 'stop', 'hello', 'hello', 'stop', 'help me', 'yes', 'help me', 'stop', 'stop', 'yes', 'yes', 'thank you', 'help me', 'thank you', 'help me', 'yes', 'thank you', 'yes', 'hello', 'stop', 'stop', 'thank you', 'stop', 'help me', 'help me', 'stop', 'yes', 'help me', 'stop', 'help me', 'yes', 'yes', 'stop', 'stop', 'help me', 'stop', 'yes', 'hello', 'hello', 'stop', 'hello', 'yes', 'stop', 'help me', 'help me', 'thank you', 'stop', 'stop', 'help me', 'stop', 'help me', 'help me', 'hello', 'stop', 'thank you', 'hello', 'stop', 'stop', 'thank you', 'stop', 'yes', 'hello', 'hello', 'thank you', 'yes', 'thank you', 'hello', 'hello', 'help me', 'yes', 'hello', 'stop', 'stop', 'help me', 'hello', 'thank you', 'help me', 'hello', 'stop', 'thank you', 'stop', 'stop', 'stop', 'hello', 'help me', 'yes', 'help me', 'stop', 'yes', 'hello', 'hello', 'yes', 'stop', 'yes', 'yes', 'stop', 'thank you', 'help me', 'thank you', 'stop', 'stop', 'hello', 'hello', 'hello', 'help me', 'help me', 'hello', 'help me', 'yes', 'yes', 'yes', 'help me', 'yes', 'stop', 'thank you', 'thank you', 'yes', 'stop', 'help me', 'yes', 'help me', 'stop', 'thank you', 'stop', 'stop', 'yes', 'thank you', 'stop', 'yes', 'yes', 'stop', 'thank you', 'hello', 'help me', 'stop', 'yes', 'hello', 'help me', 'yes', 'stop', 'hello', 'hello', 'stop', 'help me', 'help me', 'help me', 'help me', 'yes', 'hello', 'thank you', 'stop', 'stop', 'hello', 'stop', 'yes', 'stop', 'yes', 'help me', 'help me', 'thank you', 'yes', 'help me', 'stop', 'stop', 'stop', 'help me', 'stop', 'thank you', 'thank you', 'help me', 'hello', 'thank you', 'hello', 'help me', 'help me', 'yes', 'hello', 'help me', 'stop', 'thank you', 'thank you', 'yes', 'stop', 'thank you', 'yes', 'stop', 'yes', 'thank you', 'thank you', 'yes', 'yes', 'stop', 'thank you', 'hello', 'stop', 'hello', 'hello', 'yes', 'yes', 'yes', 'hello', 'thank you', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'help me', 'help me', 'hello', 'help me', 'hello', 'thank you', 'stop', 'stop', 'stop', 'thank you', 'help me', 'stop', 'stop', 'yes', 'yes', 'hello', 'thank you', 'yes', 'thank you', 'stop', 'help me', 'stop', 'help me', 'stop', 'hello', 'thank you', 'hello', 'yes', 'stop', 'yes', 'stop', 'thank you', 'thank you', 'thank you', 'hello', 'stop', 'stop', 'help me', 'hello', 'help me', 'stop', 'thank you', 'yes', 'help me', 'hello', 'stop', 'help me', 'stop', 'stop', 'yes', 'stop', 'thank you', 'yes', 'help me', 'thank you', 'hello', 'yes', 'hello', 'hello', 'stop', 'thank you', 'stop', 'help me', 'yes', 'thank you', 'hello', 'thank you', 'hello', 'thank you', 'help me', 'stop', 'help me', 'stop', 'stop', 'hello', 'stop', 'yes', 'stop', 'stop', 'thank you', 'yes', 'help me', 'hello', 'thank you', 'stop', 'stop', 'help me', 'yes', 'yes', 'thank you', 'yes', 'stop', 'thank you', 'thank you', 'stop', 'help me', 'thank you', 'stop', 'hello', 'stop', 'yes', 'stop', 'yes', 'thank you', 'help me', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'stop', 'thank you', 'hello', 'stop', 'thank you', 'stop', 'thank you', 'hello', 'yes', 'stop', 'yes', 'thank you', 'hello', 'help me', 'hello', 'help me', 'stop', 'hello', 'hello', 'help me', 'yes', 'help me', 'thank you', 'yes', 'help me', 'hello', 'thank you', 'thank you', 'stop', 'stop', 'thank you', 'help me', 'help me', 'hello', 'stop', 'thank you', 'yes', 'hello', 'yes', 'thank you', 'hello', 'yes', 'yes', 'thank you', 'stop', 'stop', 'stop', 'yes', 'help me', 'yes', 'hello', 'stop', 'thank you', 'help me', 'hello', 'thank you', 'stop', 'stop', 'thank you', 'hello', 'help me', 'hello', 'thank you', 'thank you', 'stop', 'help me', 'thank you', 'hello', 'thank you', 'help me', 'help me', 'hello', 'stop', 'thank you', 'stop', 'stop', 'hello', 'stop', 'stop', 'stop', 'stop', 'hello', 'hello', 'stop', 'thank you', 'stop', 'stop', 'yes', 'hello', 'stop', 'yes', 'stop', 'yes', 'stop', 'help me', 'yes', 'stop', 'stop', 'help me', 'help me', 'hello', 'help me', 'hello', 'hello', 'yes', 'help me', 'help me', 'stop', 'stop', 'hello', 'thank you', 'thank you', 'hello', 'thank you', 'hello', 'hello', 'thank you', 'hello', 'stop', 'stop', 'hello', 'thank you', 'help me', 'help me', 'help me', 'help me', 'yes', 'yes', 'yes', 'help me', 'help me', 'thank you', 'thank you', 'stop', 'stop', 'hello', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you', 'yes', 'help me', 'thank you', 'stop', 'thank you', 'hello', 'stop', 'yes', 'thank you', 'hello', 'yes', 'hello', 'hello', 'hello', 'stop', 'hello', 'help me', 'help me', 'stop', 'stop', 'thank you', 'help me', 'hello', 'stop', 'yes', 'hello', 'stop', 'hello', 'hello', 'stop', 'hello', 'yes', 'hello', 'thank you', 'thank you', 'thank you', 'stop', 'yes', 'hello', 'help me', 'yes', 'stop', 'thank you', 'stop', 'hello', 'yes', 'help me', 'yes', 'stop', 'thank you', 'help me', 'hello', 'stop', 'stop', 'yes', 'stop', 'hello', 'hello', 'hello', 'stop', 'yes', 'thank you', 'hello', 'thank you', 'yes', 'stop', 'stop', 'thank you', 'stop', 'yes', 'help me', 'stop', 'yes', 'help me', 'yes', 'stop', 'stop', 'stop', 'thank you', 'help me', 'thank you', 'stop', 'hello', 'yes', 'hello', 'yes', 'stop', 'stop', 'hello', 'stop', 'stop', 'stop', 'thank you', 'help me', 'help me', 'help me', 'stop', 'thank you', 'stop', 'thank you', 'stop', 'thank you', 'help me', 'stop', 'yes', 'hello', 'yes', 'yes', 'stop', 'stop', 'help me', 'hello', 'stop', 'thank you', 'help me', 'stop', 'help me', 'yes', 'hello', 'stop', 'help me', 'hello', 'yes', 'yes', 'yes', 'yes', 'stop', 'help me', 'stop', 'yes', 'stop', 'thank you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wj70u5anQXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}